#!/usr/bin/env python3
"""
Evaluation script for CLIP fine-tuned on HAR dataset
Using the OpenAI CLIP implementation
"""

import os
import sys
import argparse
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple
import warnings

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

import clip
from PIL import Image
import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    top_k_accuracy_score
)

# Import training modules
from train_clip_har import HARDataset

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CLIPHARvaluator:
    """CLIP Model Evaluator for HAR dataset"""
    
    def __init__(self, checkpoint_path: str, data_dir: str, model_name: str = 'ViT-B/32'):
        self.checkpoint_path = checkpoint_path
        self.data_dir = Path(data_dir)
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Load models
        self.load_models()
        
        # Setup test data
        self.setup_test_data()
    
    def load_models(self):
        """Load both fine-tuned and baseline CLIP models"""
        logger.info(f"Loading models...")
        
        # Load baseline model
        self.baseline_model, self.preprocess = clip.load(self.model_name, device=self.device, jit=False)
        self.baseline_model.eval()
        
        # Load fine-tuned model
        if self.checkpoint_path and os.path.exists(self.checkpoint_path):
            logger.info(f"Loading fine-tuned model from {self.checkpoint_path}")
            
            checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
            
            # Initialize fine-tuned model with same architecture
            self.finetuned_model, _ = clip.load(self.model_name, device=self.device, jit=False)\n            \n            # Load fine-tuned weights\n            self.finetuned_model.load_state_dict(checkpoint['model_state_dict'])\n            self.finetuned_model.eval()\n            \n            logger.info(f\"Loaded fine-tuned model from epoch {checkpoint['epoch']}\")\n            logger.info(f\"Checkpoint accuracy: {checkpoint.get('accuracy', 'N/A')}\")\n        else:\n            logger.warning(f\"Checkpoint not found at {self.checkpoint_path}\")\n            self.finetuned_model = None\n    \n    def setup_test_data(self):\n        \"\"\"Setup test dataset and data loader\"\"\"\n        # Try different test split names\n        test_splits = ['test', 'testing', 'testset']\n        self.test_dataset = None\n        \n        for split_name in test_splits:\n            try:\n                self.test_dataset = HARDataset(self.data_dir, split_name, preprocess=self.preprocess)\n                logger.info(f\"Using {split_name} split for evaluation\")\n                break\n            except ValueError:\n                continue\n        \n        if self.test_dataset is None:\n            # Fall back to validation split\n            try:\n                self.test_dataset = HARDataset(self.data_dir, 'val', preprocess=self.preprocess)\n                logger.info(\"Using validation split for evaluation\")\n            except ValueError:\n                raise ValueError(\"No test or validation split found in the dataset\")\n        \n        self.test_loader = DataLoader(\n            self.test_dataset, \n            batch_size=64, \n            shuffle=False, \n            num_workers=4\n        )\n        \n        self.class_names = self.test_dataset.class_names\n        self.num_classes = len(self.class_names)\n        \n        logger.info(f\"Test samples: {len(self.test_dataset)}\")\n        logger.info(f\"Number of classes: {self.num_classes}\")\n        logger.info(f\"Classes: {self.class_names}\")\n    \n    def zero_shot_classification(self, model, model_name: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform zero-shot classification using text templates\"\"\"\n        logger.info(f\"Performing zero-shot classification with {model_name}...\")\n        \n        # Create text templates for each class\n        templates = [\n            \"a photo of a person {}\",\n            \"someone {}\",\n            \"a person performing {}\",\n            \"{} action\",\n            \"human {}\",\n            \"a video frame of someone {}\",\n            \"footage of a person {}\"\n        ]\n        \n        # Generate text descriptions for all classes\n        text_descriptions = []\n        for class_name in self.class_names:\n            clean_name = class_name.replace('_', ' ').replace('-', ' ').lower()\n            class_texts = [template.format(clean_name) for template in templates]\n            text_descriptions.extend(class_texts)\n        \n        # Encode text descriptions\n        text_tokens = clip.tokenize(text_descriptions, truncate=True).to(self.device)\n        with torch.no_grad():\n            text_features = model.encode_text(text_tokens)\n            text_features = F.normalize(text_features, dim=-1)\n        \n        # Reshape text features: [num_classes, num_templates, feature_dim]\n        text_features = text_features.reshape(self.num_classes, len(templates), -1)\n        # Average across templates\n        text_features = text_features.mean(dim=1)  # [num_classes, feature_dim]\n        \n        # Evaluate on test set\n        all_predictions = []\n        all_labels = []\n        all_similarities = []\n        \n        with torch.no_grad():\n            for batch in tqdm(self.test_loader, desc=f\"Zero-shot {model_name}\"):\n                images = batch['image'].to(self.device)\n                labels = batch['class_id'].cpu().numpy()\n                \n                # Encode images\n                image_features = model.encode_image(images)\n                image_features = F.normalize(image_features, dim=-1)\n                \n                # Compute similarities\n                similarities = (image_features @ text_features.T) * model.logit_scale.exp()\n                predictions = similarities.argmax(dim=1).cpu().numpy()\n                \n                all_predictions.extend(predictions)\n                all_labels.extend(labels)\n                all_similarities.extend(similarities.cpu().numpy())\n        \n        return np.array(all_predictions), np.array(all_labels), np.array(all_similarities)\n    \n    def compute_metrics(self, predictions: np.ndarray, labels: np.ndarray, \n                       similarities: np.ndarray, model_name: str) -> Dict:\n        \"\"\"Compute evaluation metrics\"\"\"\n        # Basic accuracy\n        accuracy = accuracy_score(labels, predictions)\n        \n        # Top-k accuracy\n        top3_accuracy = top_k_accuracy_score(labels, similarities, k=min(3, self.num_classes))\n        top5_accuracy = top_k_accuracy_score(labels, similarities, k=min(5, self.num_classes))\n        \n        # Per-class metrics\n        report = classification_report(\n            labels, predictions, \n            target_names=self.class_names, \n            output_dict=True, \n            zero_division=0\n        )\n        \n        # Confusion matrix\n        cm = confusion_matrix(labels, predictions)\n        \n        metrics = {\n            'model_name': model_name,\n            'accuracy': accuracy,\n            'top3_accuracy': top3_accuracy,\n            'top5_accuracy': top5_accuracy,\n            'macro_precision': report['macro avg']['precision'],\n            'macro_recall': report['macro avg']['recall'],\n            'macro_f1': report['macro avg']['f1-score'],\n            'weighted_precision': report['weighted avg']['precision'],\n            'weighted_recall': report['weighted avg']['recall'],\n            'weighted_f1': report['weighted avg']['f1-score'],\n            'per_class_metrics': {\n                class_name: {\n                    'precision': report[class_name]['precision'],\n                    'recall': report[class_name]['recall'],\n                    'f1-score': report[class_name]['f1-score'],\n                    'support': report[class_name]['support']\n                }\n                for class_name in self.class_names if class_name in report\n            },\n            'confusion_matrix': cm.tolist()\n        }\n        \n        return metrics\n    \n    def plot_confusion_matrix(self, cm: np.ndarray, model_name: str, save_path: str = None):\n        \"\"\"Plot confusion matrix\"\"\"\n        plt.figure(figsize=(max(8, self.num_classes * 0.5), max(6, self.num_classes * 0.4)))\n        \n        # Use percentage if there are many classes\n        if self.num_classes > 10:\n            cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n            sns.heatmap(\n                cm_percent, \n                annot=True, \n                fmt='.1f', \n                cmap='Blues',\n                xticklabels=self.class_names,\n                yticklabels=self.class_names,\n                cbar_kws={'label': 'Percentage'}\n            )\n        else:\n            sns.heatmap(\n                cm, \n                annot=True, \n                fmt='d', \n                cmap='Blues',\n                xticklabels=self.class_names,\n                yticklabels=self.class_names\n            )\n        \n        plt.title(f'Confusion Matrix - {model_name}')\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            logger.info(f\"Saved confusion matrix to {save_path}\")\n        \n        plt.show()\n    \n    def plot_comparison(self, baseline_metrics: Dict, finetuned_metrics: Dict, save_path: str = None):\n        \"\"\"Plot comparison between baseline and fine-tuned models\"\"\"\n        metrics_to_compare = ['accuracy', 'top3_accuracy', 'top5_accuracy', 'macro_f1', 'weighted_f1']\n        \n        baseline_values = [baseline_metrics[metric] for metric in metrics_to_compare]\n        finetuned_values = [finetuned_metrics[metric] for metric in metrics_to_compare]\n        \n        x = np.arange(len(metrics_to_compare))\n        width = 0.35\n        \n        fig, ax = plt.subplots(figsize=(12, 6))\n        rects1 = ax.bar(x - width/2, baseline_values, width, label='Baseline CLIP', alpha=0.8)\n        rects2 = ax.bar(x + width/2, finetuned_values, width, label='Fine-tuned CLIP', alpha=0.8)\n        \n        ax.set_ylabel('Score')\n        ax.set_title('Model Performance Comparison')\n        ax.set_xticks(x)\n        ax.set_xticklabels([metric.replace('_', ' ').title() for metric in metrics_to_compare])\n        ax.legend()\n        ax.set_ylim(0, 1)\n        \n        # Add value labels on bars\n        def autolabel(rects):\n            for rect in rects:\n                height = rect.get_height()\n                ax.annotate(f'{height:.3f}',\n                            xy=(rect.get_x() + rect.get_width() / 2, height),\n                            xytext=(0, 3),\n                            textcoords=\"offset points\",\n                            ha='center', va='bottom')\n        \n        autolabel(rects1)\n        autolabel(rects2)\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            logger.info(f\"Saved comparison plot to {save_path}\")\n        \n        plt.show()\n    \n    def evaluate(self, save_dir: str = './evaluation_results'):\n        \"\"\"Run complete evaluation\"\"\"\n        save_dir = Path(save_dir)\n        save_dir.mkdir(exist_ok=True)\n        \n        logger.info(\"Starting evaluation...\")\n        \n        # Baseline evaluation\n        baseline_predictions, baseline_labels, baseline_similarities = self.zero_shot_classification(\n            self.baseline_model, \"Baseline CLIP\"\n        )\n        baseline_metrics = self.compute_metrics(\n            baseline_predictions, baseline_labels, baseline_similarities, \"Baseline CLIP\"\n        )\n        \n        results = {\n            'baseline_metrics': baseline_metrics\n        }\n        \n        # Fine-tuned evaluation (if available)\n        if self.finetuned_model is not None:\n            finetuned_predictions, finetuned_labels, finetuned_similarities = self.zero_shot_classification(\n                self.finetuned_model, \"Fine-tuned CLIP\"\n            )\n            finetuned_metrics = self.compute_metrics(\n                finetuned_predictions, finetuned_labels, finetuned_similarities, \"Fine-tuned CLIP\"\n            )\n            \n            results['finetuned_metrics'] = finetuned_metrics\n            results['improvement'] = {\n                'accuracy': finetuned_metrics['accuracy'] - baseline_metrics['accuracy'],\n                'macro_f1': finetuned_metrics['macro_f1'] - baseline_metrics['macro_f1'],\n                'weighted_f1': finetuned_metrics['weighted_f1'] - baseline_metrics['weighted_f1']\n            }\n        \n        # Print results\n        logger.info(\"\\n\" + \"=\"*60)\n        logger.info(\"EVALUATION RESULTS\")\n        logger.info(\"=\"*60)\n        logger.info(f\"Baseline CLIP Accuracy: {baseline_metrics['accuracy']:.4f}\")\n        logger.info(f\"Baseline Top-3 Accuracy: {baseline_metrics['top3_accuracy']:.4f}\")\n        logger.info(f\"Baseline Top-5 Accuracy: {baseline_metrics['top5_accuracy']:.4f}\")\n        logger.info(f\"Baseline Macro F1: {baseline_metrics['macro_f1']:.4f}\")\n        logger.info(f\"Baseline Weighted F1: {baseline_metrics['weighted_f1']:.4f}\")\n        \n        if self.finetuned_model is not None:\n            logger.info(f\"\\nFine-tuned CLIP Accuracy: {finetuned_metrics['accuracy']:.4f}\")\n            logger.info(f\"Fine-tuned Top-3 Accuracy: {finetuned_metrics['top3_accuracy']:.4f}\")\n            logger.info(f\"Fine-tuned Top-5 Accuracy: {finetuned_metrics['top5_accuracy']:.4f}\")\n            logger.info(f\"Fine-tuned Macro F1: {finetuned_metrics['macro_f1']:.4f}\")\n            logger.info(f\"Fine-tuned Weighted F1: {finetuned_metrics['weighted_f1']:.4f}\")\n            \n            logger.info(f\"\\nImprovements:\")\n            logger.info(f\"Accuracy: {results['improvement']['accuracy']:+.4f}\")\n            logger.info(f\"Macro F1: {results['improvement']['macro_f1']:+.4f}\")\n            logger.info(f\"Weighted F1: {results['improvement']['weighted_f1']:+.4f}\")\n        \n        # Save results\n        with open(save_dir / 'evaluation_results.json', 'w') as f:\n            json.dump(results, f, indent=2)\n        \n        # Plot confusion matrices\n        self.plot_confusion_matrix(\n            np.array(baseline_metrics['confusion_matrix']),\n            \"Baseline CLIP\",\n            save_path=save_dir / 'confusion_matrix_baseline.png'\n        )\n        \n        if self.finetuned_model is not None:\n            self.plot_confusion_matrix(\n                np.array(finetuned_metrics['confusion_matrix']),\n                \"Fine-tuned CLIP\",\n                save_path=save_dir / 'confusion_matrix_finetuned.png'\n            )\n            \n            # Plot comparison\n            self.plot_comparison(\n                baseline_metrics, finetuned_metrics,\n                save_path=save_dir / 'model_comparison.png'\n            )\n        \n        # Save detailed classification reports\n        with open(save_dir / 'baseline_classification_report.txt', 'w') as f:\n            f.write(\"Baseline CLIP Classification Report\\n\")\n            f.write(\"=\"*50 + \"\\n\\n\")\n            f.write(classification_report(\n                baseline_labels, baseline_predictions, \n                target_names=self.class_names\n            ))\n        \n        if self.finetuned_model is not None:\n            with open(save_dir / 'finetuned_classification_report.txt', 'w') as f:\n                f.write(\"Fine-tuned CLIP Classification Report\\n\")\n                f.write(\"=\"*50 + \"\\n\\n\")\n                f.write(classification_report(\n                    finetuned_labels, finetuned_predictions, \n                    target_names=self.class_names\n                ))\n        \n        logger.info(f\"\\nResults saved to {save_dir}\")\n        \n        return results\n\ndef main():\n    parser = argparse.ArgumentParser(description='Evaluate CLIP models on HAR dataset')\n    parser.add_argument('--data_dir', type=str, required=True,\n                        help='Path to HAR dataset directory')\n    parser.add_argument('--checkpoint', type=str,\n                        help='Path to fine-tuned CLIP checkpoint (optional)')\n    parser.add_argument('--model_name', type=str, default='ViT-B/32',\n                        help='CLIP model name')\n    parser.add_argument('--save_dir', type=str, default='./evaluation_results',\n                        help='Directory to save evaluation results')\n    \n    args = parser.parse_args()\n    \n    # Create evaluator and run evaluation\n    evaluator = CLIPHAREvaluator(args.checkpoint, args.data_dir, args.model_name)\n    results = evaluator.evaluate(args.save_dir)\n    \n    print(\"\\nEvaluation completed!\")\n    print(f\"Results saved to: {args.save_dir}\")\n\nif __name__ == '__main__':\n    main()"


